{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "introduction_to_pyspark.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMaWaxhqmAqK344RQJEVu3H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SurajKande/Pipelining/blob/master/introduction_to_pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3MwrTKfQGUa",
        "colab_type": "text"
      },
      "source": [
        "Spark is a platform for cluster computing. Spark lets you spread data and computations over clusters with multiple nodes (think of each node as a separate computer). Splitting up your data makes it easier to work with very large datasets because each node only works with a small amount of data.\n",
        "\n",
        "As each node works on its own subset of the total data, it also carries out a part of the total calculations required, so that both data processing and computation are performed in parallel over the nodes in the cluster. It is a fact that parallel computation can make certain types of programming tasks much faster.\n",
        "\n",
        "* The first step in using Spark is connecting to a cluster.\n",
        "  > the cluster will be hosted on a remote machine that's connected to all other nodes. There will be one computer, called the master that manages splitting up the data and the computations. The master is connected to the rest of the computers in the cluster, which are called worker. The master sends the workers data and calculations to run, and they send their results back to the master.\n",
        "\n",
        "  -  to connect to a Spark cluster from PySpark Create an instance of the `SparkContext` class.\n",
        "\n",
        "<br> \n",
        "* Spark's core data structure is the Resilient Distributed Dataset (RDD), he Spark DataFrame was designed to behave a lot like a SQL table\n",
        "\n",
        "  - To start working with Spark DataFrames, you first have to create a `SparkSession` object from your `SparkContext`.\n",
        "  \n",
        "  -  You can think of the `SparkContext` as your connection to the cluster and the `SparkSession` as your interface with that connection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muMOnYPKwT6N",
        "colab_type": "text"
      },
      "source": [
        "### Getting to know PySpark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbOdNfsBhfXC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "ddbdc3f2-1660-4d90-d166-3ddb1aa1efe8"
      },
      "source": [
        "# to connect gdrive to colab for importing dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount = True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y57kVcT_P6Ew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install pyspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YKHOqZ1UXjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark import SparkContext as sc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRF4cUlYU0Mf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16e91bf1-fbbc-44fd-d637-e7281e7e8d6d"
      },
      "source": [
        "print(sc)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pyspark.context.SparkContext'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbeJABXph9U7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "baa82669-2679-4f01-9963-54fed4ebc038"
      },
      "source": [
        "print(sc.version)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<property object at 0x7f3e5e85b908>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aAY9pftVOZw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae6b74f4-207f-4a94-8f8b-e48e90157ef1"
      },
      "source": [
        "# creating a spark session\n",
        "# Import SparkSession from pyspark.sql\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create my_spark\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Print my_spark\n",
        "print(spark)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7f3e5e103dd8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2rQo9_mjmY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flights_dataframe = spark.read.options(header=True).csv(\"/content/gdrive/My Drive/datasets/flights_small.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcV6PL98qNfB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "124698c4-27f0-431b-d296-ff76be5b03c8"
      },
      "source": [
        "flights_dataframe"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[year: string, month: string, day: string, dep_time: string, dep_delay: string, arr_time: string, arr_delay: string, carrier: string, tailnum: string, flight: string, origin: string, dest: string, air_time: string, distance: string, hour: string, minute: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLGRkjyEqyDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# converted the dataframe into table\n",
        "flights_dataframe.createOrReplaceTempView(\"flights\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiwh6XvvqP0x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "acc1658d-1d77-46d0-8a9d-89a5cc58c646"
      },
      "source": [
        "# Don't change this query\n",
        "query = \"FROM flights SELECT * LIMIT 10\"\n",
        "\n",
        "# Get the first 10 rows of flights\n",
        "flights10 = spark.sql(query)\n",
        "\n",
        "# Show the results\n",
        "flights10.show()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
            "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
            "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
            "|2014|   12|  8|     658|       -7|     935|       -5|     VX| N846VA|  1780|   SEA| LAX|     132|     954|   6|    58|\n",
            "|2014|    1| 22|    1040|        5|    1505|        5|     AS| N559AS|   851|   SEA| HNL|     360|    2677|  10|    40|\n",
            "|2014|    3|  9|    1443|       -2|    1652|        2|     VX| N847VA|   755|   SEA| SFO|     111|     679|  14|    43|\n",
            "|2014|    4|  9|    1705|       45|    1839|       34|     WN| N360SW|   344|   PDX| SJC|      83|     569|  17|     5|\n",
            "|2014|    3|  9|     754|       -1|    1015|        1|     AS| N612AS|   522|   SEA| BUR|     127|     937|   7|    54|\n",
            "|2014|    1| 15|    1037|        7|    1352|        2|     WN| N646SW|    48|   PDX| DEN|     121|     991|  10|    37|\n",
            "|2014|    7|  2|     847|       42|    1041|       51|     WN| N422WN|  1520|   PDX| OAK|      90|     543|   8|    47|\n",
            "|2014|    5| 12|    1655|       -5|    1842|      -18|     VX| N361VA|   755|   SEA| SFO|      98|     679|  16|    55|\n",
            "|2014|    4| 19|    1236|       -4|    1508|       -7|     AS| N309AS|   490|   SEA| SAN|     135|    1050|  12|    36|\n",
            "|2014|   11| 19|    1812|       -3|    2352|       -4|     AS| N564AS|    26|   SEA| ORD|     198|    1721|  18|    12|\n",
            "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLWIMFx_rrQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Don't change this query\n",
        "query = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n",
        "\n",
        "# Run the query\n",
        "flight_counts = spark.sql(query)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI3-BeqmseTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the results to a pandas DataFrame\n",
        "flight_pandas_dataframe_counts = flight_counts.toPandas()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvcWCZHnthft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo apt install openjdk-8-jdk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCAH1DNUtwxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo update-alternatives --config java"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fANyqz_8tfSA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "2ac35723-a28e-4062-d812-07ce04bed2b8"
      },
      "source": [
        "!java -version"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_252\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~18.04-b09)\n",
            "OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJm1Z7ZEvpmB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "3f1fdd72-4076-4f4d-9309-c7243a4a9b25"
      },
      "source": [
        "file_path = \"/content/gdrive/My Drive/datasets/airports.csv\"\n",
        "\n",
        "# Read in the airports data\n",
        "airports = spark.read.csv(file_path, header=True)\n",
        "\n",
        "# Show the data\n",
        "airports.show()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+--------------------+----------------+-----------------+----+---+---+\n",
            "|faa|                name|             lat|              lon| alt| tz|dst|\n",
            "+---+--------------------+----------------+-----------------+----+---+---+\n",
            "|04G|   Lansdowne Airport|      41.1304722|      -80.6195833|1044| -5|  A|\n",
            "|06A|Moton Field Munic...|      32.4605722|      -85.6800278| 264| -5|  A|\n",
            "|06C| Schaumburg Regional|      41.9893408|      -88.1012428| 801| -6|  A|\n",
            "|06N|     Randall Airport|       41.431912|      -74.3915611| 523| -5|  A|\n",
            "|09J|Jekyll Island Air...|      31.0744722|      -81.4277778|  11| -4|  A|\n",
            "|0A9|Elizabethton Muni...|      36.3712222|      -82.1734167|1593| -4|  A|\n",
            "|0G6|Williams County A...|      41.4673056|      -84.5067778| 730| -5|  A|\n",
            "|0G7|Finger Lakes Regi...|      42.8835647|      -76.7812318| 492| -5|  A|\n",
            "|0P2|Shoestring Aviati...|      39.7948244|      -76.6471914|1000| -5|  U|\n",
            "|0S9|Jefferson County ...|      48.0538086|     -122.8106436| 108| -8|  A|\n",
            "|0W3|Harford County Ai...|      39.5668378|      -76.2024028| 409| -5|  A|\n",
            "|10C|  Galt Field Airport|      42.4028889|      -88.3751111| 875| -6|  U|\n",
            "|17G|Port Bucyrus-Craw...|      40.7815556|      -82.9748056|1003| -5|  A|\n",
            "|19A|Jackson County Ai...|      34.1758638|      -83.5615972| 951| -4|  U|\n",
            "|1A3|Martin Campbell F...|      35.0158056|      -84.3468333|1789| -4|  A|\n",
            "|1B9| Mansfield Municipal|      42.0001331|      -71.1967714| 122| -5|  A|\n",
            "|1C9|Frazier Lake Airpark|54.0133333333333|-124.768333333333| 152| -8|  A|\n",
            "|1CS|Clow Internationa...|      41.6959744|      -88.1292306| 670| -6|  U|\n",
            "|1G3|  Kent State Airport|      41.1513889|      -81.4151111|1134| -4|  A|\n",
            "|1OH|     Fortman Airport|      40.5553253|      -84.3866186| 885| -5|  U|\n",
            "+---+--------------------+----------------+-----------------+----+---+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0QJMY0IwWBx",
        "colab_type": "text"
      },
      "source": [
        "### Manipulating data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf9ZkWSGwbR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}