{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ETL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMlmdFahhydyZCnxQ13ed8b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SurajKande/Pipelining/blob/master/ETL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m12C-EkHmOZZ",
        "colab_type": "text"
      },
      "source": [
        "#ETL : Extract, Transform and Load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mQLjyTomQtl",
        "colab_type": "text"
      },
      "source": [
        "##Extract:\n",
        "\n",
        "- this means extracting data from Persistent storage, which is not suited for data processing,\n",
        "\n",
        "- persistent storage could be a file from S3 or a SQL databasse\n",
        "\n",
        "- It is necessary stage before transforming the data\n",
        "\n",
        "- The sources to extract from vary\n",
        "    * we can extract data from text files, example\n",
        "        - plain text or paragraphs like in textbook\n",
        "        - or flat files like csv,tsv\n",
        "        - JSON files : hold infomation in semi structured way\n",
        "             * we can map JSON objects to python dictionaries using json module \n",
        "        - etc. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFyXiQiWmShj",
        "colab_type": "code",
        "outputId": "6b6a2abd-fb26-475e-c0c2-c847b180b053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Fetch from an API\n",
        "import requests\n",
        "\n",
        "# Fetch the Hackernews post\n",
        "resp = requests.get(\"https://hacker-news.firebaseio.com/v0/item/16222426.json\")\n",
        "\n",
        "# Print the response parsed as JSON\n",
        "print(resp.json())\n",
        "\n",
        "# Assign the score of the test to post_score\n",
        "post_score = resp.json()['score']\n",
        "print(post_score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'by': 'neis', 'descendants': 0, 'id': 16222426, 'score': 17, 'time': 1516800333, 'title': 'Duolingo-Style Learning for Data Science: DataCamp for Mobile', 'type': 'story', 'url': 'https://medium.com/datacamp/duolingo-style-learning-for-data-science-datacamp-for-mobile-3861d1bc02df'}\n",
            "17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXzZbtG80pHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to extract table to a pandas DataFrame       \n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "import sqlalchemy\n",
        "\n",
        "def extract_table_to_pandas(tablename, db_engine):\n",
        "    query = \"SELECT * FROM {}\".format(tablename)\n",
        "    return pd.read_sql(query, db_engine)\n",
        "\n",
        "# Connect to the database using the connection URI\n",
        "connection_uri = \"postgresql://repl:password@localhost:5432/pagila\"       \n",
        "db_engine = sqlalchemy.create_engine(connection_uri)\n",
        "\n",
        "# Extract the film table into a pandas DataFrame\n",
        "film_dataframe = extract_table_to_pandas(\"film\", db_engine)\n",
        "\n",
        "# Extract the customer table into a pandas DataFrame\n",
        "customer_dataframe = extract_table_to_pandas(\"customer\", db_engine)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6AOgE4E1i6a",
        "colab_type": "text"
      },
      "source": [
        "##Tansform:\n",
        "   its nonexhaustive list to perform on data during Transform phase some of them are:\n",
        "   -  Data validation\n",
        "   -  Translation of code values ( e.g. NEWYORK -->  NY  )\n",
        "   -  splitting \n",
        "   -  joining from various sources\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_WFwWvfg0h6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the rental rate column as a string\n",
        "rental_rate_str = film_dataframe.rental_rate.astype(str)\n",
        "\n",
        "# Split up and expand the column\n",
        "rental_rate_expanded = rental_rate_str.str.split('.', expand=True)\n",
        "\n",
        "# Assign the columns to film_df\n",
        "film_dataframe = film_dataframe.assign(\n",
        "    rental_rate_dollar =rental_rate_expanded[0],\n",
        "    rental_rate_cents =rental_rate_expanded[1],\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe_lnamjlsVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query = \"SELECT film.film_id, film.rating, customer.customer_id FROM film, customer\"\n",
        "rating_dataframe = pd.read_sql(query,db_engine)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GzWqUJMhM2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use groupBy and mean to aggregate the column\n",
        "ratings_per_film_dataframe = rating_dataframe.groupBy('film_id').mean('rating')\n",
        "\n",
        "# Join the tables using the film_id column\n",
        "film_dataframe_with_ratings = film_dataframe.join(\n",
        "    ratings_per_film_dataframe,\n",
        "    film_dataframe.film_id==ratings_per_film_dataframe.film_id\n",
        ")\n",
        "\n",
        "# Show the 5 first results\n",
        "print(film_dataframe_with_ratings.show(5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnmwApYBnZOu",
        "colab_type": "text"
      },
      "source": [
        "## Loading\n",
        " To load the extracted and transformed data into analytics database for analyzing the data by data scientists \n",
        "\n",
        " -  files are often loaded into a MPP( massive parallel processing ) database like Redshift in order to make it available for analysis.\n",
        "\n",
        " -  typically MPP databases load data best from files that use columnar storage format\n",
        "\n",
        " -  we use file format called parquet for this purpose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AY5ZSoJor4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write the pandas DataFrame to parquet\n",
        "film_dataframe.to_parquet(\"films_pdf.parquet\")\n",
        "\n",
        "# Write the PySpark DataFrame to parquet\n",
        "film_sparkdataframe.write.parquet(\"films_sdf.parquet\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR6rNKbyq3fx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finish the connection URI\n",
        "connection_uri = \"postgresql://repl:password@localhost:5432/data_warehouse\"\n",
        "db_engine_data_warehouse = sqlalchemy.create_engine(connection_uri)\n",
        "\n",
        "# Transformation step, join with recommendations data\n",
        "film_dataframe_joined = film_dataframe.join(recommendations)\n",
        "\n",
        "# Finish the .to_sql() call to write to store.film\n",
        "film_dataframe_joined.to_sql(\"film\",db_engine_data_warehouse, schema=\"store\", if_exists=\"replace\")\n",
        "\n",
        "# Run the query to fetch the data\n",
        "pd.read_sql(\"SELECT film_id, recommended_film_ids FROM store.film\", db_engine_data_warehouse)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}