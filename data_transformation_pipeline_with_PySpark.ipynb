{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_transformation_pipeline_with_PySpark.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM4/I+wo2e+XXD5roezMY8r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SurajKande/Pipelining/blob/master/data_transformation_pipeline_with_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rek4uLm50GDv",
        "colab_type": "text"
      },
      "source": [
        "* spark is an analytics engine for processing large quantities of data\n",
        "\n",
        "* to load the dataset with pyspark we must create a spark session  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BkL5mIrz4A1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYvDn-2k2Fn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read a csv file and set the headers\n",
        "ratings_dataframe = (spark.read.options(header=True).csv(\"ratings.csv\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dec0Nys75rou",
        "colab_type": "text"
      },
      "source": [
        "if we do not define a schema, all column values will be parsed as strings which can be inefficient to process. You are usually better off defining the data types in a schema yourself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXPkBjyC3Ip0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the schema\n",
        "schema = StructType([\n",
        "  StructField(\"brand\", StringType(), nullable=False),\n",
        "  StructField(\"model\", StringType(), nullable=False),\n",
        "  StructField(\"absorption_rate\", ByteType(), nullable=True),\n",
        "  StructField(\"comfort\",ByteType() , nullable=True)\n",
        "])\n",
        "\n",
        "ratings_dataframe = (spark.read.options(header=\"true\").schema(schema).csv(\"ratings.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUaCIpDtYG_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify the option to drop invalid rows\n",
        "ratings_dataframe = spark.read.options(header=True, mode=\"DROPMALFORMED\").csv(\"ratings.csv\"))\n",
        "\n",
        "\n",
        "# Replace nulls with arbitrary value on column subset\n",
        "ratings_dataframe = ratings_dataframe.fillna(4, subset=[\"comfort\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BckmeDLlwlJH",
        "colab_type": "text"
      },
      "source": [
        "* Transformations are, after ingestion, the next step in data engineering pipelines. Data gets transformed, because certain insights need to be derived. Clear column names help in achieving that goal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E2xeQsTwmdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Selecting and renaming columns\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "result_rating_dataframe = ratings_dataframe.select([col(\"brand\"), \n",
        "                                   col(\"model\"),\n",
        "                                   col(\"absorption_rate\").alias(\"absorbency\")])\n",
        "\n",
        "# only unique values\n",
        "result_rating_dataframe_unique_values = result_rating_dataframe.distinct()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0OQUviBxAFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Grouping and aggregating data\n",
        "from pyspark.sql.functions import col, avg, stddev_samp, max as sfmax\n",
        "\n",
        "aggregated_rating_dataframe = (purchased\n",
        "                                # Group rows by 'Country'\n",
        "                               .groupBy(col('Country'))\n",
        "                               .agg(\n",
        "                                     # Calculate the average salary\n",
        "                                     avg('Salary').alias('average_salary'),\n",
        "                                     # Calculate the standard deviation \n",
        "                                     stddev_samp('Salary'),\n",
        "                                     # Calculate highest salary\n",
        "                                     sfmax('Salary').alias('highest_salary')\n",
        "                                  )\n",
        "                                )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROQRl5kW2CpC",
        "colab_type": "text"
      },
      "source": [
        "### Using the spark-submit :\n",
        "  \n",
        "  1. sets up launch environment for use with the cluster manager and the selected deploy mode\n",
        "\n",
        "  2. invokes main class/app/module/function \n",
        "\n",
        "\n",
        "#### Creating a deployable artifact:\n",
        "\n",
        "to run a PySpark program locally, first zip your code. This packaging step becomes more important when your code consists of many modules.\n",
        "\n",
        "* navigate to the root folder of your pipeline run the following command:\n",
        "\n",
        " ``` zip --recurse-paths zip_file.zip pipeline_folder ```\n",
        "\n",
        " \n",
        "\n",
        "* to run a PySpark application locally:\n",
        "\n",
        "    ``` spark-submit --py-files <PY_FILES> <MAIN_PYTHON_FILE> ```\n",
        "     - PY_FILES:  being either a zipped archive\n",
        "     - MAIN_PYTHON_FILE:   should be the entry point of your application."
      ]
    }
  ]
}