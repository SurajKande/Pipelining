{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Workflow scheduling frameworks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOJFQzAOWcTtKUvk0kQLerM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SurajKande/Pipelining/blob/master/Workflow_scheduling_frameworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7vAEML7Vxsa",
        "colab_type": "text"
      },
      "source": [
        "Its the task of the scheduling frameworks to orchestarte the job of parallel computing \n",
        "\n",
        "exmaple:\n",
        "consider a spark job that pulls data from a csv file, cleans the records and loads data for analysis, and it needs to it weekly as new data gets added to the csv file\n",
        "     \n",
        "   * one way is to manually  \n",
        "   * use tools like cron in linux\n",
        "   * then what about dependencies like job3 should run only after job2 and it shold run after only job1 ?\n",
        "\n",
        "The tools for the job\n",
        "  - Linux's cron\n",
        "  - Spotify's Luigi\n",
        "  - ApacheAirow    \n",
        "  \n",
        "  the above tools use [DAG( directed acycic graphs)](https://airflow.apache.org/docs/stable/concepts.html)\n",
        "    - Set of nodes\n",
        "    - Directed edges\n",
        "    - No cycles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDekbzmzVOc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install apache-airflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R31Q7oUJZ7Ns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from airflow import DAG "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sblGd_kaVlb",
        "colab_type": "text"
      },
      "source": [
        "consider an A fictional DAG of car factory simulation\n",
        "\n",
        "![car_factory_simulation](https://assets.datacamp.com/production/repositories/5000/datasets/44f52c1b25308c762f24dcde116b62e275ce7fe1/DAG.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axCQA2ypVb69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dag = DAG(dag_id=\"car_factory_simulation\",\n",
        "          default_args={\"owner\": \"airflow\",\"start_date\": airflow.utils.dates.days_ago(2)},\n",
        "          schedule_interval=\"0 * * * *\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwFmPZlcb1Mf",
        "colab_type": "text"
      },
      "source": [
        "we use Operators to define the jobs, several kinds of Operators exist simple ones are \n",
        "\n",
        "  * bashOperator, PythonOperator that execute bash or python code respectively\n",
        "  * there are ways to write your own operator like SparkJobOperator or StartClusterOperator "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ctdj52L2aMn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Task definitions\n",
        "from airflow.operators.bash_operator import BashOperator\n",
        "\n",
        "assemble_frame = BashOperator(task_id=\"assemble_frame\", bash_command='echo \"Assembling frame\"', dag=dag)\n",
        "place_tires = BashOperator(task_id=\"place_tires\", bash_command='echo \"Placing tires\"', dag=dag)\n",
        "assemble_body = BashOperator(task_id=\"assemble_body\", bash_command='echo \"Assembling body\"', dag=dag)\n",
        "apply_paint = BashOperator(task_id=\"apply_paint\", bash_command='echo \"Applying paint\"', dag=dag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgOnfj7aavyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set up dependency flow\n",
        "assemble_frame.set_downstream(place_tires)\n",
        "assemble_frame.set_downstream(assemble_body)\n",
        "assemble_body.set_downstream(apply_paint)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}