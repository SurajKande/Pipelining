{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Parallel_computation _frameworks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN4LteR5ro5fAwidpjEuLzC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SurajKande/Pipelining/blob/master/Parallel_computation_frameworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBMfNWmBG9G6",
        "colab_type": "text"
      },
      "source": [
        "### Hadoop:\n",
        "- It is a collection of open source projects, maintained by Apache Software Foundation\n",
        "-  two main hadoop projects\n",
        "      \n",
        "      * MapReduce\n",
        "\n",
        "          -  it works by splliting the program tasks into subtasks, distributing the workload and data between several processing units \n",
        "          ( units are several computers in a clusters)\n",
        "\n",
        "          - it was hard to write these MapReduce jobs\n",
        "\n",
        "      * HDFS  \n",
        "          \n",
        "          - is a distributed file system, similar to the file system on computer only differenceis the files reside on different systems, now a days it is replaced by cloud strage systems ex: AWS S3 \n",
        "\n",
        "\n",
        "### Hive:\n",
        "\n",
        "   -  It is a layer on top of Hadoop ecosystem that makes data from several sources queryable in a structured way using Hive SQL. \n",
        "\n",
        "\n",
        "\n",
        "### Spark:\n",
        "\n",
        "   -  It distributes data processing tasks between clusters of computers\n",
        "\n",
        "   -  Spark architecture relies on resilient distributed datasets ( RDD ) \n",
        "\n",
        "      *  It has a datastructure that maintains data which is distributed between multiple nodes\n",
        "\n",
        "      *  It dosent have names columns, similar to list of tuples\n",
        "\n",
        "      *  we can perform transformations like, result in transformed RDD's\n",
        "          \n",
        "           - .map()\n",
        "           - .filter()\n",
        "      *   we can perform actions like, result in singlr result\n",
        "\n",
        "            - .count()\n",
        "            - .first()\n",
        "        \n",
        "      *   people typically use PySpark is Python interface to spark, there are also other interfaces in other languages like R, Scala\n",
        "\n",
        "            - PySpark has DataFrame abstraction, (i.e) we can do operrations very similar to pandas DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_PFzloyPcJE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB0w6y9RPtdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.1/spark-2.4.1-bin-hadoop2.7.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quS812R-Piu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar xf spark-2.4.1-bin-hadoop2.7.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q_zXgtIPnAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q findspark pyspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYUw2W4qQXd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-MZ6pwgRB_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to connect gdrive to colab for importing dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H93DoUpjMnNL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "0d84a66e-950a-43d0-ca19-69e0f4c8827c"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "if __name__ == \"__main__\": \n",
        "  spark = SparkSession.builder.getOrCreate()\n",
        "  athlete_events_spark_dataframe = (spark.read.csv('/content/gdrive/My Drive/datasets/athlete_events.csv',inferSchema=True, header=True, escape='\"'))\n",
        "  \n",
        "  athlete_events_spark_dataframe = (athlete_events_spark_dataframe.withColumn(\"Age\", athlete_events_spark_dataframe.Age.cast(\"integer\")))\n",
        "  print(athlete_events_spark_dataframe.groupBy('Year').mean('Age').orderBy('Year').show())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+------------------+\n",
            "|Year|          avg(Age)|\n",
            "+----+------------------+\n",
            "|1896|23.580645161290324|\n",
            "|1900|29.034031413612567|\n",
            "|1904| 26.69814995131451|\n",
            "|1906|27.125252525252524|\n",
            "|1908|26.970228384991845|\n",
            "|1912| 27.53861997940268|\n",
            "|1920|29.290977661734843|\n",
            "|1924|28.373324544056253|\n",
            "|1928| 29.11255692908263|\n",
            "|1932| 32.58207957204948|\n",
            "|1936|27.530328324986087|\n",
            "|1948|28.783946700507613|\n",
            "|1952|26.161546085232903|\n",
            "|1956|25.926673567977915|\n",
            "|1960|25.168848457954294|\n",
            "|1964| 24.94439728353141|\n",
            "|1968|24.248045555448314|\n",
            "|1972| 24.30860659192447|\n",
            "|1976|23.841818181818184|\n",
            "|1980|23.694742857142856|\n",
            "+----+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_JYqQfgGGRW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "77207d61-d4cd-4efc-f9cf-75818da5070e"
      },
      "source": [
        "# Print the type of athlete_events_spark\n",
        "print(type(athlete_events_spark_dataframe))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJVCH3I3SFg0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "40199835-8b95-4b53-d200-4d2bf8230b33"
      },
      "source": [
        "# Print the schema of athlete_events_spark\n",
        "print(athlete_events_spark_dataframe.printSchema())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- ID: integer (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Sex: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Height: string (nullable = true)\n",
            " |-- Weight: string (nullable = true)\n",
            " |-- Team: string (nullable = true)\n",
            " |-- NOC: string (nullable = true)\n",
            " |-- Games: string (nullable = true)\n",
            " |-- Year: integer (nullable = true)\n",
            " |-- Season: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- Sport: string (nullable = true)\n",
            " |-- Event: string (nullable = true)\n",
            " |-- Medal: string (nullable = true)\n",
            "\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8BmjPdoSHdM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26b991e6-3e84-4312-94a1-a52adbceafc6"
      },
      "source": [
        "# Group by the Year, and find the mean Age\n",
        "print(athlete_events_spark_dataframe.groupBy('Year').mean(\"Age\"))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataFrame[Year: int, avg(Age): double]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZCKifO1SJbw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "dcd300c1-878d-4d8c-a51c-52d2b89f2869"
      },
      "source": [
        "# Group by the Year, and find the mean Age\n",
        "print(athlete_events_spark_dataframe.groupBy('Year').mean(\"Age\").show())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+------------------+\n",
            "|Year|          avg(Age)|\n",
            "+----+------------------+\n",
            "|1896|23.580645161290324|\n",
            "|1924|28.373324544056253|\n",
            "|2006|25.959151072569604|\n",
            "|1908|26.970228384991845|\n",
            "|1952|26.161546085232903|\n",
            "|1956|25.926673567977915|\n",
            "|1988|24.079431552931485|\n",
            "|1994|24.422102596580114|\n",
            "|1968|24.248045555448314|\n",
            "|2014|25.987323655694134|\n",
            "|1904| 26.69814995131451|\n",
            "|2004|25.639514989213716|\n",
            "|1932| 32.58207957204948|\n",
            "|1996|24.915045018878885|\n",
            "|1998|25.163197335553704|\n",
            "|1960|25.168848457954294|\n",
            "|2012| 25.96137770897833|\n",
            "|1912| 27.53861997940268|\n",
            "|2016| 26.20791934541204|\n",
            "|1936|27.530328324986087|\n",
            "+----+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}